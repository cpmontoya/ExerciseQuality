---
title: "Exercise Activity Evaluation Analysis"
output: html_document
---

This is an analysis of data collected to test the ability to classify the quality of activity performance. Links to the original data and a paper describing the experiment can be found at <http://groupware.les.inf.puc-rio.br/har>.   

## Exploring and Pre-processing the Data
A subset of the data was made available to the class for training and testing as well as a small set of data to submit predictions for grading. The first step was to load the data and take a look. 
```{r}
rawdata<-read.csv("pml-training.csv",head=TRUE)
analysisdata<-rawdata[,-grep("kurtosis|skewness|max|min|avg|stddev|var|amplitude|X|window|user|timestamp",names(rawdata))]
```
Many of the fields have a majority of blanks or NA Values. They correspond to statistics such as mean and standard deviations calculated over varying time windows. These values will not be very useful for the predictions of classe, there are very few actual values and the data to evaluate for grading do not have any values for these fields. The user and time stamp data will not be useful either since the quality of exercise should not be related to the actual user per se or to the absolute time of the data. The time stamps could be useful for the relating the various observations, but again the data for submission is not sufficient to create time window statistics so all of these fields have been removed.

```{r}
library(caret,quietly=TRUE)
split<-.1
set.seed(203289)
inBuild<- createDataPartition(y=analysisdata$classe,p=split,list=FALSE)
validation<- analysisdata[-inBuild,]
builddata<-analysisdata[inBuild,]
inTrain<-createDataPartition(y=builddata$classe,p=split,list=FALSE)
training<-builddata[inTrain,]
testing<-builddata[-inTrain,]
classeIndex<-grep("classe",names(training))
#Figure 1
featurePlot(training[,seq(1,classeIndex-1,13)],y=training$classe,plot="pairs",auto.key = list(columns = 5))
corMat<-abs(cor(training[,-classeIndex]))
diag(corMat) = 0
highCor<-which(corMat>0.9, arr.ind=T)
highCor<-highCor[order(highCor[,1]),]
#Figure 2
featurePlot(training[,c(1,4,8:10)],y=training$classe,plot="pairs",auto.key = list(columns = 5))
```
Looking through the featurePlots in pairs in Figure 1, there are a number of predictor combinations that show different structures for different classe values. They do not look even roughly linear however, so a tree algorithm such as random forests is likely to work better and more simply than linear regression models. In Figure 2 it seems that the classe values overlap considerably for the accel predictors and since they are highly correlated (over 90%) with other variables removing them should not hurt the analysis. The gyro predictors are also highly correlated but with each other rather than other predictors. 
```{r}
subSample<-createDataPartition(y=training$classe,p=.2,list=FALSE)
modelFit <- train(training[subSample,]$classe~.,method="rf",data=training[subSample,-classeIndex],prox=TRUE)
important<-varImp(modelFit)$importance
training<-training[,-grep("accel",names(training))]
testing<-testing[,-grep("accel",names(testing))]
validation<-validation[,-grep("accel",names(validation))]
classeIndex<-grep("classe",names(training))
```
Performing a random forest classification on a small subset of the data confirms that the accel variables are not the highest contributors to the classification. The most important predictor appears to be the roll_belt. Removing the accel predictors leaves `r dim(training)[2]`. The gyro predictors are also not strong contributors and as noted above they are strongly correlated with each other. Perhaps PCA could be done on a subset of predictors that are strongly correlated (over 70%) to further reduce the predictors needed.
```{r}
#exclude roll_Belt and classe
corMat<-abs(cor(training[,-c(1,classeIndex)]))
diag(corMat) = 0
highCor<-which(corMat>0.5, arr.ind=T)
highCor<-highCor[order(highCor[,1]),]
highCor<-unique(highCor[,1])+1 #take unique indicies and add one since roll_Belt was excluded
preProc <- preProcess(training[,highCor],method="pca",thresh=.95)

trainPC <- predict(preProc,training[,highCor])
training<-training[,-highCor]
training[names(trainPC)]<-trainPC
classeIndex<-grep("classe",names(training))
classeInBack<- c(c(1:dim(training)[2])[-classeIndex],classeIndex)
training<-training[,classeInBack]

testPC <- predict(preProc,testing[,highCor])
testing<-testing[,-highCor]
testing[names(testPC)]<-testPC
testing<-testing[,classeInBack]

valPC <- predict(preProc,validation[,highCor])
validation<-validation[,-highCor]
validation[names(valPC)]<-valPC
validation<-validation[,classeInBack]
```
## Using Cross Validation to weed out more predictors
Checking on which are the least important contributors to the accuracy on the training set we can systematically eliminate them and look at the accuracy on the testing set. Choosing the elimination scheme with best accuracy on the test set helps reduce the possibility of overfitting. I am only showing three to save time for presentation

```{r}
classeIndex<-grep("classe",names(training))
modelFit <- train(training$classe~.,method="rf",data=training[,-classeIndex],prox=TRUE)
predictions<-predict(modelFit,testing[,-classeIndex])
confusionMatrix(predictions,testing$classe)
rmI<-c()
vAcc<-confusionMatrix(predictions,testing$classe)$overall[1]

important<-varImp(modelFit)$importance
leastImport<-order(important['Overall'])
bounds<- 1*1:3+4

for(i in bounds){
     removeIndexes <- -c(leastImport[1:i],classeIndex)
     modelFitNw <- train(training$classe~.,method="rf",data=training[,removeIndexes],prox=TRUE)
     predictions<-predict(modelFitNw,testing[,removeIndexes])
     confusionMatrix(predictions,testing$classe)
     if (confusionMatrix(predictions,testing$classe)$overall[1]>max(vAcc)){
          modelFit<-modelFitNw
          rmI<-removeIndexes
     }
     vAcc<-append(vAcc,confusionMatrix(predictions,testing$classe)$overall[1])
}
vAcc
i<-which.max(vAcc)
i
removeIndexes <- rmI
```
## Estimate of Accuracy 
Taking the best choice of predictors determined from the previous step we can apply the fit to the validation set which the model has not seen yet. 
```{r}
#predictions<-predict(modelFit,validation[,removeIndexes])
#confusionMatrix(predictions,validation$classe)
```
The estimate on accuracy is `r confusionMatrix(predictions,validation$classe)$overall[1]`. I needed to comment out the validation code and Ideally I would run with more data in the training set to achieve better accuracy as I did for submission but I was getting a script error with the Rmarkdown for the validation set that was not showing up in the regular script